Logistic Regression
"Pclass", "Sex" -> 78.680000000000007
"Pclass", "Sex", "CabinLetter" -> 78.680000000000007
"Pclass", "Sex", "Age" -> 79.909999999999997
"Pclass", "Sex", "AgeRange" -> 79.909999999999997 (train)  0.75598 (test)
"Pclass", "Sex", "AgeRange", "Title" -> 80.129999999999995 (train) 0.76555 (test)
"Pclass", "Sex", "AgeRange", "Title", 'CabinLetter' -> 80.920000000000002 (train) 0.75598 (test)
"Pclass", "Sex", "AgeRange", "Title", 'CabinLetter', 'Embarked' -> 80.129999999999995 (train) 0.76076 (test)
"Pclass", "Sex", "AgeRange", "Title", 'Embarked' -> 80.019999999999996 (train)
"Pclass", "Sex", "AgeRange", "Title", 'CabinLetter', 'Embarked', 'FamilyMems' -> 81.030000000000001 (train) 0.77033 (test)
"Pclass", "Sex", "AgeRange", "Title", 'CabinLetter', 'Embarked', 'FamilyMems', 'Fare' -> 80.469999999999999 (train) 0.75598 (test)

Random Forest
"Pclass", "Sex", "AgeRange", "Title", 'CabinLetter', 'Embarked', 'FamilyMems', 'Fare' -> 91.579999999999998 (train) 0.77990 (test)




I've now split 20% of the data into a dev set

Logistic Regression
"Pclass", "Sex", "AgeRange", "Title", 'CabinLetter', 'Embarked', 'FamilyMems', 'Fare' -> 82.159999999999997 (train)

Random Forest
"Pclass", "Sex", "AgeRange", "Title", 'CabinLetter', 'Embarked', 'FamilyMems', 'Fare' -> 91.290000000000006 (train)
{'criterion': 'entropy', 'max_depth': None, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 10} -> 89.189999999999998 (train) 92.310000000000002 (dev) 0.77033 (test)

Started using K folds because this I think it should be a better estimate than just one dev Set

{'criterion': 'entropy', 'max_depth': None, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 10} -> 83.73283395755307 (KFolds) 0.77033 (test)
{'criterion': 'gini', 'max_depth': 30, 'max_features': 'log2', 'min_samples_leaf': 3, 'min_samples_split': 3, 'n_estimators': 10} -> 82.83520599250936 (KFolds) 0.75598 (test)


AdaBoost
Standard features No hyper changes -> 83.150000000000006 (train) 81.010000000000005 (dev) 81.48439450686642 (KFolds) 0.74162 (test)
{n_estimators=10} -> 83.290000000000006 (train) 81.010000000000005 (test) 81.93133583021224 (KFolds) 0.75598 (test)

Added variable "IsAlone"

Random Forest
"Pclass", "Sex", "AgeRange", "Title", 'CabinLetter', 'Embarked', 'FamilyMems', 'Fare', 'IsAlone' -> 90.730000000000004 (train) 82.120000000000005 (dev) 81.93258426966292 (KFolds)
{'criterion': 'entropy', 'max_depth': 30, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 30, 'n_estimators': 10} -> 85.390000000000001 (train) 82.120000000000005 (dev) 83.16978776529338 (KFolds) 0.77990 (test)

Encoded variables to dummy variables
With Random Forest default -> 90.730000000000004 (train) 82.680000000000007 (dev) 83.16978776529338 (KFolds) 0.76076 (test)
{'criterion': 'gini', 'max_depth': 1000, 'max_features': 'log2', 'min_samples_leaf': 3, 'min_samples_split': 3, 'n_estimators': 10} -> 85.390000000000001 (train) 81.560000000000002 (dev) 83.16978776529338 (KFolds) 0.77511 (test)
