Logistic Regression
"Pclass", "Sex" -> 78.680000000000007
"Pclass", "Sex", "CabinLetter" -> 78.680000000000007
"Pclass", "Sex", "Age" -> 79.909999999999997
"Pclass", "Sex", "AgeRange" -> 79.909999999999997 (train)  0.75598 (test)
"Pclass", "Sex", "AgeRange", "Title" -> 80.129999999999995 (train) 0.76555 (test)
"Pclass", "Sex", "AgeRange", "Title", 'CabinLetter' -> 80.920000000000002 (train) 0.75598 (test)
"Pclass", "Sex", "AgeRange", "Title", 'CabinLetter', 'Embarked' -> 80.129999999999995 (train) 0.76076 (test)
"Pclass", "Sex", "AgeRange", "Title", 'Embarked' -> 80.019999999999996 (train)
"Pclass", "Sex", "AgeRange", "Title", 'CabinLetter', 'Embarked', 'FamilyMems' -> 81.030000000000001 (train) 0.77033 (test)
"Pclass", "Sex", "AgeRange", "Title", 'CabinLetter', 'Embarked', 'FamilyMems', 'Fare' -> 80.469999999999999 (train) 0.75598 (test)

Random Forest
"Pclass", "Sex", "AgeRange", "Title", 'CabinLetter', 'Embarked', 'FamilyMems', 'Fare' -> 91.579999999999998 (train) 0.77990 (test)




I've now split 20% of the data into a CV set

Logistic Regression
"Pclass", "Sex", "AgeRange", "Title", 'CabinLetter', 'Embarked', 'FamilyMems', 'Fare' -> 82.159999999999997 (train)

Random Forest
"Pclass", "Sex", "AgeRange", "Title", 'CabinLetter', 'Embarked', 'FamilyMems', 'Fare' -> 91.290000000000006 (train)
{'criterion': 'entropy', 'max_depth': None, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 10} -> 89.189999999999998 (train) 92.310000000000002 (CV) 0.77033 (test)

Started using K folds because this I think it should be a better estimate than just one CV Set
{'criterion': 'entropy', 'max_depth': None, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 10} -> 83.73283395755307 (KFolds) 0.77033 (test)
{'criterion': 'gini', 'max_depth': 30, 'max_features': 'log2', 'min_samples_leaf': 3, 'min_samples_split': 3, 'n_estimators': 10} -> 82.83520599250936 (KFolds) 0.75598 (test)
